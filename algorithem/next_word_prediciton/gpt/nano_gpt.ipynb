{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP wiht LSTM TensorFlow Youtube Tutorial \n",
    "\n",
    "Code: https://medium.com/@saipragna.kancheti/nanogpt-a-small-scale-gpt-for-text-generation-in-pytorch-tensorflow-and-jax-641c4efefbd5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_extractor(url = \"https://www.gutenberg.org/files/11/11-0.txt\"):\n",
    "    # Request to fetch the Alice dataset\n",
    "    response = requests.get(url)\n",
    "    # Checking if we got a valid response\n",
    "    if response.status_code == 200:\n",
    "        # Opening a file and writing the content of the response\n",
    "        with open('input.txt', 'w') as file:\n",
    "            file.write(response.text)\n",
    "    else:\n",
    "        print(f\"Failed to get file with status code: {response.status_code}\")\n",
    "    # Reading the downloaded file\n",
    "    with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "        text = f.read()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_encoder(text):\n",
    "    # Listing and sorting the unique characters in the text\n",
    "    chars = sorted(list(set(text)))\n",
    "    # Getting the total number of unique characters\n",
    "    vocab_size = len(chars)\n",
    "    print(\"\".join(chars))\n",
    "    print(vocab_size)\n",
    "    # Creating mappings from characters to their corresponding numerical representations\n",
    "    stoi = {ch:i for i, ch in enumerate(chars)}\n",
    "    # Creating mappings from numbers to their corresponding characters\n",
    "    itos = {i:ch for i, ch in enumerate(chars)}\n",
    "    # Function to encode a string into a list of numbers\n",
    "    encode = lambda s: [stoi[ch] for ch in s]\n",
    "    # Function to decode a list of numbers back into a string\n",
    "    decode = lambda l: \"\".join([itos[i] for i in l])\n",
    "    print(encode(\"hii this is pragna\"))\n",
    "    print(\"decoded: \", decode(encode(\"hii this is pragna\")))\n",
    "    # Encoding the entire text into numbers\n",
    "    series = encode(text)\n",
    "    n = int(0.8*len(series))\n",
    "    return series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(layers.Layer):\n",
    "    def __init__(self, config):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "\n",
    "        self.num_heads = config.n_head\n",
    "        self.head_size = config.n_embd // config.n_head\n",
    "\n",
    "        # Projecting input into key, query, and value for all attention heads, but in batch\n",
    "        self.c_attn = layers.Dense(3 * config.n_embd, use_bias=config.bias)\n",
    "\n",
    "        # Regularization\n",
    "        self.attn_dropout = layers.Dropout(config.dropout)\n",
    "        self.resid_dropout = layers.Dropout(config.dropout)\n",
    "\n",
    "    def call(self, x):\n",
    "        B, T, C = x.shape\n",
    "\n",
    "        # Linear transformation for queries, keys, and values, note that C = n_embd\n",
    "        qkv = self.c_attn(x)  # Input shape: (B, T, C), Output shape: (B, T, 3 * n_embd)\n",
    "\n",
    "        # Split the queries, keys, and values\n",
    "        q, k, v = tf.split(qkv, 3, axis=-1)  # Input shape: (B, T, 3 * n_embd), Output shapes: 3 * (B, T, n_embd)\n",
    "\n",
    "\n",
    "        # Reshape queries, keys, and values for multi-head attention with head_size = n_embd // num_heads\n",
    "        # BUG: possible issue with tensorflow, you can use tf.reshape(q, (B, T, self.num_heads, -1)), for tensorflow B is unknown: it will give an error\n",
    "        q = tf.reshape(q, (-1, T, self.num_heads, self.head_size))  # Output shape: (B, T, num_heads, head_size)\n",
    "        k = tf.reshape(k, (-1, T, self.num_heads, self.head_size))  # Output shape: (B, T, num_heads, head_size)\n",
    "        v = tf.reshape(v, (-1, T, self.num_heads, self.head_size))  # Output shape: (B, T, num_heads, head_size)\n",
    "\n",
    "\n",
    "        # Perform attention operations\n",
    "\n",
    "        # Transpose queries, keys, and values for efficient matrix multiplication\n",
    "        q = tf.transpose(q, perm=[0, 2, 1, 3])  # Output shape: (B, num_heads, T, head_size)\n",
    "        k = tf.transpose(k, perm=[0, 2, 1, 3])  # Output shape: (B, num_heads, T, head_size)\n",
    "        v = tf.transpose(v, perm=[0, 2, 1, 3])  # Output shape: (B, num_heads, T, head_size)\n",
    "\n",
    "        # Compute attention scores (\"affinities\")\n",
    "        wei = tf.matmul(q, k, transpose_b=True) * (self.head_size ** -0.5)  # Output shape: (B, num_heads, T, T)\n",
    "\n",
    "        mask = tf.linalg.band_part(tf.ones_like(wei), -1, 0)  # Lower triangular matrix of ones\n",
    "        wei = tf.where(mask == 1, wei, float(\"-inf\"))  # Set upper triangular part to -inf\n",
    "\n",
    "        wei = tf.nn.softmax(wei, axis=-1)  # Output shape: (B, num_heads, T, T)\n",
    "        wei = self.attn_dropout(wei)  # Regularization step 1\n",
    "\n",
    "        # Perform the weighted aggregation of the values\n",
    "        out = tf.matmul(wei, v)  # Output shape: (B, num_heads, T, head_size)\n",
    "\n",
    "        # Transpose and reshape the output to match the original shape\n",
    "        out = tf.transpose(out, perm=[0, 2, 1, 3])  # Output shape: (B, T, num_heads, head_size)\n",
    "        out = tf.reshape(out, (-1, T, C))  # Output shape: (B, T, C) - note that C = num_heads * head_size = n_embd\n",
    "        out = self.resid_dropout(out)  # Regularization step 2\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(layers.Layer):\n",
    "    def __init__(self, config):\n",
    "        super(MLP, self).__init__()\n",
    "        n_embed = config.n_embd\n",
    "        self.c_fc = layers.Dense(4 * n_embed, use_bias=config.bias, activation=tf.keras.activations.gelu)\n",
    "        self.c_proj = layers.Dense(config.n_embd, use_bias=config.bias)\n",
    "        self.dropout = layers.Dropout(config.dropout)\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.c_fc(x)\n",
    "        x = self.c_proj(x)\n",
    "        x = self.dropout(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(layers.Layer):\n",
    "    def __init__(self, config):\n",
    "        super(Block, self).__init__()\n",
    "\n",
    "        # Layer normalizing the input data as the number of features increases over time\n",
    "        self.ln_1 = layers.LayerNormalization(epsilon=config.epsilon, center=False, scale=True)\n",
    "        self.attn = MultiHeadAttention(config)\n",
    "        self.ln_2 = layers.LayerNormalization(epsilon=config.epsilon, center=False, scale=True)\n",
    "        self.mlp = MLP(config)\n",
    "\n",
    "    def call(self, x):\n",
    "        # 1. Input data is layer normalized: Layer normalizing the input data as the number of features increases over time\n",
    "        x_normalized = self.ln_1(x)\n",
    "\n",
    "        # 2. Fed through the attention network: We get the attention scores or weighted values\n",
    "        attn_output = self.attn(x_normalized)\n",
    "\n",
    "        # 3. Added to the input: Reduces vanishing gradient issues\n",
    "        x = x + attn_output\n",
    "\n",
    "        # 4. Layer normalized the data again\n",
    "        x_normalized = self.ln_2(x)\n",
    "\n",
    "        # 5. Final pass through a multi-layer perceptron: We are learning the features\n",
    "        mlp_output = self.mlp(x_normalized)\n",
    "\n",
    "        # 6. Added to the input again\n",
    "        x = x + mlp_output\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoder(config):\n",
    "    \"\"\"\n",
    "    Creates an decoder model based on the provided configuration.\n",
    "\n",
    "    Args:\n",
    "        config: An object specifying the configuration parameters.\n",
    "\n",
    "    Returns:\n",
    "        decoder: A Keras Model object representing the encoder model.\n",
    "    \"\"\"\n",
    "\n",
    "    # create a dict with all the layers we need\n",
    "    transformer_dict = {\n",
    "        # input layer\n",
    "        'input': tf.keras.Input(shape=(config.block_size,)),\n",
    "        # word token embeddings\n",
    "        'wte': tf.keras.layers.Embedding(config.vocab_size, config.n_embd, input_length=config.block_size),\n",
    "        # word position embeddings\n",
    "        'wpe': tf.keras.layers.Embedding(config.block_size, config.n_embd),\n",
    "        # dropout layer\n",
    "        'drop': tf.keras.layers.Dropout(config.dropout),\n",
    "        # Transformer blocks\n",
    "        'h': tf.keras.Sequential([Block(config) for _ in range(config.n_layer)]),\n",
    "        # layer normalization\n",
    "        'ln_f': tf.keras.layers.LayerNormalization(epsilon=config.epsilon, center=False, scale=True),\n",
    "        # layer used to project the output of the GPT model to the vocabulary size\n",
    "        'lm_head': tf.keras.layers.Dense(config.vocab_size, use_bias=False)\n",
    "    }\n",
    "\n",
    "    # input\n",
    "    idx = transformer_dict['input']\n",
    "    pos = tf.range(0, config.block_size, dtype=tf.int64)  # shape (t)\n",
    "\n",
    "    # Forward the GPT model itself\n",
    "    tok_emb = transformer_dict['wte'](idx)  # token embeddings of shape (b, t, n_embd)\n",
    "    pos_emb = transformer_dict['wpe'](pos)  # position embeddings of shape (t, n_embd)\n",
    "    x = transformer_dict['drop'](tok_emb + pos_emb)\n",
    "    for block in transformer_dict['h'].layers:\n",
    "        x = block(x)\n",
    "    x = transformer_dict['ln_f'](x)\n",
    "\n",
    "    # logit scores for each vocabulary word at each position in the input sequence.\n",
    "    logits = transformer_dict['lm_head'](x)  # shape (batch_size, sequence_length, vocab_size)\n",
    "\n",
    "    # Create encoder model\n",
    "    model = tf.keras.Model(inputs=idx, outputs=logits, name='encoder')\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def windowed_dataset(series, window_size, batch_size, shuffle_buffer):\n",
    "    # Creating a tensorflow dataset from the encoded series\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(series)\n",
    "    # Creating a windowed dataset with each window of size window_size + 1 and shifting the window by 1 after each step\n",
    "    dataset = dataset.window(size=window_size+1, shift = 1, drop_remainder=True)\n",
    "    # Flattening the dataset\n",
    "    dataset = dataset.flat_map(lambda window: window.batch(window_size+1))\n",
    "    # Splitting each window into features (all elements except the last) and target (the last element)\n",
    "    dataset = dataset.map(lambda x: (x[:-1], x[1:]))\n",
    "    # Shuffling the dataset\n",
    "    dataset = dataset.shuffle(shuffle_buffer)\n",
    "    # Batching the dataset and prefetching 1 batch at a time to improve performance\n",
    "    dataset = dataset.batch(batch_size).prefetch(1)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'config' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Create the decoder model\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m decoder_model \u001b[38;5;241m=\u001b[39m decoder(\u001b[43mconfig\u001b[49m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Compile and train the model\u001b[39;00m\n\u001b[1;32m      5\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39moptimizers\u001b[38;5;241m.\u001b[39mAdam(learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'config' is not defined"
     ]
    }
   ],
   "source": [
    "# Create the decoder model\n",
    "decoder_model = decoder(config)\n",
    "\n",
    "# Compile and train the model\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "epochs = 10\n",
    "\n",
    "decoder_model.compile(optimizer=optimizer, loss=loss_fn)\n",
    "history = decoder_model.fit(train_dataset, epochs=epochs, validation_data=test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fine_tune_optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
    "\n",
    "# Continue training the model for fine-tuning\n",
    "fine_tune_epochs = 5\n",
    "\n",
    "decoder_model.compile(optimizer=fine_tune_optimizer, loss=loss_fn)\n",
    "history_fine_tune = decoder_model.fit(\n",
    "    train_dataset,\n",
    "    epochs=fine_tune_epochs,\n",
    "    validation_data=test_dataset,\n",
    "    callbacks=[early_stopping]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, tokenizer, initial_prompt, generation_length=100):\n",
    "    generated_text = initial_prompt\n",
    "    for _ in range(generation_length):\n",
    "        # Convert the current prompt into model inputs\n",
    "        input_ids = tokenizer.encode(generated_text)\n",
    "\n",
    "        # Pad or truncate the input sequence to match the model's expected input length\n",
    "        if len(input_ids) > config.block_size:\n",
    "            input_ids = input_ids[-config.block_size:]\n",
    "        else:\n",
    "            padding_length = config.block_size - len(input_ids)\n",
    "            input_ids = [0] * padding_length + input_ids  # Prepend zeros for padding\n",
    "\n",
    "        input_ids = tf.expand_dims(input_ids, 0)\n",
    "\n",
    "        # Get predictions for the next token\n",
    "        logits = model(input_ids)\n",
    "        predictions = logits[:, -1, :]\n",
    "\n",
    "        # Sample the output (using tf.random.categorical) to generate token ID\n",
    "        token_id = tf.random.categorical(predictions, num_samples=1)[0, 0].numpy()\n",
    "\n",
    "        # Convert token ID back to character\n",
    "        token = tokenizer.decode([token_id])\n",
    "\n",
    "        # Append the token to the generated text\n",
    "        generated_text += token\n",
    "\n",
    "    return generated_text"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
