{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next Word Prediction using GPT 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Predict-Next-Word Example Using Hugging Face and GPT-2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-  [Quelle 1](https://jamesmccaffrey.wordpress.com/2021/10/21/a-predict-next-word-example-using-hugging-face-and-gpt-2/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip install torch\n",
    "#! pip install transformers\n",
    "#! pip install numpy \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next word prediciton "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Begin next-word using HF GPT-2 demo \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d603fd4d4dd5480da391fe5a30b60475",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1390ee6df43c4bd490a0472c0e51c223",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96f6d165eefa4479b7d38f5e6627b07c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "300bf838b51a474d940bd6cb2d6e79f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1bf28fc524914e30bce8efb6446f699a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-14 12:54:57.610532: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-11-14 12:55:00.073463: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-11-14 12:55:00.073662: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-11-14 12:55:00.526064: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-11-14 12:55:01.522796: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-11-14 12:55:05.711299: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "133e5840c97c422c82c3e2b3c808b471",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8743a75691834f0ca96ddf1e6cdb739c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Input sequence: \n",
      "Machine learning with PyTorch can do amazing\n",
      "\n",
      "Tokenized input data structure: \n",
      "{'input_ids': tensor([[37573,  4673,   351,  9485, 15884,   354,   460,   466,  4998]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "\n",
      "Token IDs and their words: \n",
      "tensor(37573) Machine\n",
      "tensor(4673)  learning\n",
      "tensor(351)  with\n",
      "tensor(9485)  Py\n",
      "tensor(15884) Tor\n",
      "tensor(354) ch\n",
      "tensor(460)  can\n",
      "tensor(466)  do\n",
      "tensor(4998)  amazing\n",
      "\n",
      "All logits for next word: \n",
      "tensor([[-114.9652, -118.0908, -123.3014,  ..., -124.5989, -127.7998,\n",
      "         -118.4347]])\n",
      "torch.Size([1, 50257])\n",
      "\n",
      "Predicted token ID of next word: \n",
      "1243\n",
      "\n",
      "Predicted next word for sequence: \n",
      " things\n",
      "\n",
      "End demo \n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "#from torch import nn\n",
    "import numpy as np\n",
    "\n",
    "print(\"\\nBegin next-word using HF GPT-2 demo \")\n",
    "\n",
    "toker = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
    "\n",
    "seq = \"Machine learning with PyTorch can do amazing\"\n",
    "print(\"\\nInput sequence: \")\n",
    "print(seq)\n",
    "\n",
    "inpts = toker(seq, return_tensors=\"pt\")\n",
    "print(\"\\nTokenized input data structure: \")\n",
    "print(inpts)\n",
    "\n",
    "inpt_ids = inpts[\"input_ids\"]  # just IDS, no attn mask\n",
    "print(\"\\nToken IDs and their words: \")\n",
    "for id in inpt_ids[0]:\n",
    "  word = toker.decode(id)\n",
    "  print(id, word)\n",
    "\n",
    "with torch.no_grad():\n",
    "  logits = model(**inpts).logits[:, -1, :]\n",
    "print(\"\\nAll logits for next word: \")\n",
    "print(logits)\n",
    "print(logits.shape)\n",
    "\n",
    "pred_id = torch.argmax(logits).item()\n",
    "print(\"\\nPredicted token ID of next word: \")\n",
    "print(pred_id)\n",
    "\n",
    "pred_word = toker.decode(pred_id)\n",
    "print(\"\\nPredicted next word for sequence: \")\n",
    "print(pred_word)\n",
    "\n",
    "print(\"\\nEnd demo \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Top 10 predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Begin next-word using HF GPT-2 demo \n",
      "\n",
      "Input sequence: \n",
      "i am \n",
      "\n",
      "Tokenized input data structure: \n",
      "{'input_ids': tensor([[ 72, 716, 220]]), 'attention_mask': tensor([[1, 1, 1]])}\n",
      "\n",
      "Token IDs and their words: \n",
      "tensor(72) i\n",
      "tensor(716)  am\n",
      "tensor(220)  \n",
      "\n",
      "All logits for next word: \n",
      "tensor([[-59.5626, -60.9692, -63.4266,  ..., -70.3573, -67.7269, -64.5714]])\n",
      "torch.Size([1, 50257])\n",
      "\n",
      "Top 10 predicted token IDs for the next word: \n",
      "[488, 1134, 425, 522, 1849, 576, 544, 2474, 29773, 528]\n",
      "\n",
      "Top 10 predicted next words for the sequence: \n",
      "['ich', 'ik', 'ive', 'ike', '\\xa0', 'ile', 'ia', '!\"', '�', 'iz']\n",
      "\n",
      "End demo \n"
     ]
    }
   ],
   "source": [
    "print(\"\\nBegin next-word using HF GPT-2 demo \")\n",
    "\n",
    "# Lade GPT-2 Model und Tokenizer\n",
    "toker = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
    "\n",
    "seq = \"i am \"\n",
    "print(\"\\nInput sequence: \")\n",
    "print(seq)\n",
    "\n",
    "# Tokenisiere den Eingabesatz\n",
    "inpts = toker(seq, return_tensors=\"pt\")\n",
    "print(\"\\nTokenized input data structure: \")\n",
    "print(inpts)\n",
    "\n",
    "inpt_ids = inpts[\"input_ids\"]  # Nur die Token-IDs\n",
    "print(\"\\nToken IDs and their words: \")\n",
    "for id in inpt_ids[0]:\n",
    "    word = toker.decode(id)\n",
    "    print(id, word)\n",
    "\n",
    "# Berechne die Logits\n",
    "with torch.no_grad():\n",
    "    logits = model(**inpts).logits[:, -1, :]\n",
    "print(\"\\nAll logits for next word: \")\n",
    "print(logits)\n",
    "print(logits.shape)\n",
    "\n",
    "# Top-10 wahrscheinlichste Wörter auswählen\n",
    "top_k = 10\n",
    "top_k_probs = torch.topk(logits, top_k)\n",
    "top_k_ids = top_k_probs.indices[0].tolist()\n",
    "\n",
    "print(f\"\\nTop {top_k} predicted token IDs for the next word: \")\n",
    "print(top_k_ids)\n",
    "\n",
    "print(f\"\\nTop {top_k} predicted next words for the sequence: \")\n",
    "top_k_words = [toker.decode(pred_id) for pred_id in top_k_ids]\n",
    "print(top_k_words)\n",
    "\n",
    "print(\"\\nEnd demo \")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Top 10 predicted in German"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Begin next-word prediction using German GPT-2 model\n",
      "\n",
      "Input sequence: \n",
      "ikannst du mir etwas \n",
      "\n",
      "Tokenized input data structure: \n",
      "{'input_ids': tensor([[ 424,  390,  268,  671,  943, 1410,  225]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1]])}\n",
      "\n",
      "Token IDs and their words: \n",
      "tensor(424) ik\n",
      "tensor(390) ann\n",
      "tensor(268) st\n",
      "tensor(671)  du\n",
      "tensor(943)  mir\n",
      "tensor(1410)  etwas\n",
      "tensor(225)  \n",
      "\n",
      "All logits for next word: \n",
      "torch.Size([1, 50265])\n",
      "\n",
      "Top 10 predicted token IDs for the next word: \n",
      "[2462, 5664, 1353, 871, 824, 140, 5822, 16503, 2663, 4090]\n",
      "\n",
      "Top 10 predicted next words for the sequence: \n",
      "['reiben', 'riech', 'ruck', 'öff', 'reis', '�', 'uff', 'räu', 'tiger', 'reibung']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"\\nBegin next-word prediction using German GPT-2 model\")\n",
    "\n",
    "# Lade den deutschen GPT-2 Tokenizer und das Modell\n",
    "toker = AutoTokenizer.from_pretrained(\"dbmdz/german-gpt2\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"dbmdz/german-gpt2\")\n",
    "\n",
    "# Beispiel-Eingabesequenz auf Deutsch\n",
    "seq = \"kannst du mir etwas \"\n",
    "print(\"\\nInput sequence: \")\n",
    "print(seq)\n",
    "\n",
    "# Tokenisiere den Text\n",
    "inpts = toker(seq, return_tensors=\"pt\")\n",
    "print(\"\\nTokenized input data structure: \")\n",
    "print(inpts)\n",
    "\n",
    "# Token-IDs anzeigen\n",
    "inpt_ids = inpts[\"input_ids\"]\n",
    "print(\"\\nToken IDs and their words: \")\n",
    "for id in inpt_ids[0]:\n",
    "    word = toker.decode(id)\n",
    "    print(id, word)\n",
    "\n",
    "# Vorhersage für das nächste Token\n",
    "with torch.no_grad():\n",
    "    logits = model(**inpts).logits[:, -1, :]\n",
    "print(\"\\nAll logits for next word: \")\n",
    "print(logits.shape)\n",
    "\n",
    "# Top-10 wahrscheinlichste Wörter auswählen\n",
    "top_k = 10\n",
    "top_k_probs = torch.topk(logits, top_k)\n",
    "top_k_ids = top_k_probs.indices[0].tolist()\n",
    "\n",
    "print(f\"\\nTop {top_k} predicted token IDs for the next word: \")\n",
    "print(top_k_ids)\n",
    "\n",
    "# Top-10 Wörter dekodieren\n",
    "print(f\"\\nTop {top_k} predicted next words for the sequence: \")\n",
    "top_k_words = [toker.decode(pred_id) for pred_id in top_k_ids]\n",
    "print(top_k_words)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
