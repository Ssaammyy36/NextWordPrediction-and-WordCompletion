{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next Word Prediction using GPT 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Predict-Next-Word Example Using Hugging Face and GPT-2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-  [Quelle 1](https://jamesmccaffrey.wordpress.com/2021/10/21/a-predict-next-word-example-using-hugging-face-and-gpt-2/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip install torch\n",
    "#! pip install transformers\n",
    "#! pip install numpy \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next word prediciton "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Begin next-word using HF GPT-2 demo \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38c1d494f17a416693b3173c48970761",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b81fffe1086c480c95c6b77e046073f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc34698426684c788e8eb5907de000fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5be59f0eb9347c9aa3ebbb0b0bb3fcf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9596524eb93143f48fe12bf4886328f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d095707600ff444f8120169de5d5182c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7948526404524389968679026816e945",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Input sequence: \n",
      "Machine learning with PyTorch can do amazing\n",
      "\n",
      "Tokenized input data structure: \n",
      "{'input_ids': tensor([[37573,  4673,   351,  9485, 15884,   354,   460,   466,  4998]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "\n",
      "Token IDs and their words: \n",
      "tensor(37573) Machine\n",
      "tensor(4673)  learning\n",
      "tensor(351)  with\n",
      "tensor(9485)  Py\n",
      "tensor(15884) Tor\n",
      "tensor(354) ch\n",
      "tensor(460)  can\n",
      "tensor(466)  do\n",
      "tensor(4998)  amazing\n",
      "\n",
      "All logits for next word: \n",
      "tensor([[-114.9652, -118.0908, -123.3014,  ..., -124.5989, -127.7998,\n",
      "         -118.4347]])\n",
      "torch.Size([1, 50257])\n",
      "\n",
      "Predicted token ID of next word: \n",
      "1243\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-08 11:17:05.414988: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-10-08 11:17:05.803976: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-10-08 11:17:05.971436: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-10-08 11:17:06.021253: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-10-08 11:17:06.293339: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-10-08 11:17:09.278793: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predicted next word for sequence: \n",
      " things\n",
      "\n",
      "End demo \n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "#from torch import nn\n",
    "import numpy as np\n",
    "\n",
    "print(\"\\nBegin next-word using HF GPT-2 demo \")\n",
    "\n",
    "toker = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
    "\n",
    "seq = \"Machine learning with PyTorch can do amazing\"\n",
    "print(\"\\nInput sequence: \")\n",
    "print(seq)\n",
    "\n",
    "inpts = toker(seq, return_tensors=\"pt\")\n",
    "print(\"\\nTokenized input data structure: \")\n",
    "print(inpts)\n",
    "\n",
    "inpt_ids = inpts[\"input_ids\"]  # just IDS, no attn mask\n",
    "print(\"\\nToken IDs and their words: \")\n",
    "for id in inpt_ids[0]:\n",
    "  word = toker.decode(id)\n",
    "  print(id, word)\n",
    "\n",
    "with torch.no_grad():\n",
    "  logits = model(**inpts).logits[:, -1, :]\n",
    "print(\"\\nAll logits for next word: \")\n",
    "print(logits)\n",
    "print(logits.shape)\n",
    "\n",
    "pred_id = torch.argmax(logits).item()\n",
    "print(\"\\nPredicted token ID of next word: \")\n",
    "print(pred_id)\n",
    "\n",
    "pred_word = toker.decode(pred_id)\n",
    "print(\"\\nPredicted next word for sequence: \")\n",
    "print(pred_word)\n",
    "\n",
    "print(\"\\nEnd demo \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Top 10 predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Begin next-word using HF GPT-2 demo \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Input sequence: \n",
      "Machine learning with PyTorch can do amazing\n",
      "\n",
      "Tokenized input data structure: \n",
      "{'input_ids': tensor([[37573,  4673,   351,  9485, 15884,   354,   460,   466,  4998]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "\n",
      "Token IDs and their words: \n",
      "tensor(37573) Machine\n",
      "tensor(4673)  learning\n",
      "tensor(351)  with\n",
      "tensor(9485)  Py\n",
      "tensor(15884) Tor\n",
      "tensor(354) ch\n",
      "tensor(460)  can\n",
      "tensor(466)  do\n",
      "tensor(4998)  amazing\n",
      "\n",
      "All logits for next word: \n",
      "tensor([[-114.9652, -118.0908, -123.3014,  ..., -124.5989, -127.7998,\n",
      "         -118.4347]])\n",
      "torch.Size([1, 50257])\n",
      "\n",
      "Top 10 predicted token IDs for the next word: \n",
      "[1243, 670, 3404, 1517, 35664, 15910, 11, 8861, 1693, 2482]\n",
      "\n",
      "Top 10 predicted next words for the sequence: \n",
      "[' things', ' work', ' stuff', ' thing', ' feats', ' tricks', ',', ' tasks', ' job', ' results']\n",
      "\n",
      "End demo \n"
     ]
    }
   ],
   "source": [
    "print(\"\\nBegin next-word using HF GPT-2 demo \")\n",
    "\n",
    "# Lade GPT-2 Model und Tokenizer\n",
    "toker = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
    "\n",
    "seq = \"Machine learning with PyTorch can do amazing\"\n",
    "print(\"\\nInput sequence: \")\n",
    "print(seq)\n",
    "\n",
    "# Tokenisiere den Eingabesatz\n",
    "inpts = toker(seq, return_tensors=\"pt\")\n",
    "print(\"\\nTokenized input data structure: \")\n",
    "print(inpts)\n",
    "\n",
    "inpt_ids = inpts[\"input_ids\"]  # Nur die Token-IDs\n",
    "print(\"\\nToken IDs and their words: \")\n",
    "for id in inpt_ids[0]:\n",
    "    word = toker.decode(id)\n",
    "    print(id, word)\n",
    "\n",
    "# Berechne die Logits\n",
    "with torch.no_grad():\n",
    "    logits = model(**inpts).logits[:, -1, :]\n",
    "print(\"\\nAll logits for next word: \")\n",
    "print(logits)\n",
    "print(logits.shape)\n",
    "\n",
    "# Top-10 wahrscheinlichste Wörter auswählen\n",
    "top_k = 10\n",
    "top_k_probs = torch.topk(logits, top_k)\n",
    "top_k_ids = top_k_probs.indices[0].tolist()\n",
    "\n",
    "print(f\"\\nTop {top_k} predicted token IDs for the next word: \")\n",
    "print(top_k_ids)\n",
    "\n",
    "print(f\"\\nTop {top_k} predicted next words for the sequence: \")\n",
    "top_k_words = [toker.decode(pred_id) for pred_id in top_k_ids]\n",
    "print(top_k_words)\n",
    "\n",
    "print(\"\\nEnd demo \")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Top 10 predicted in German"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Begin next-word prediction using German GPT-2 model\n",
      "\n",
      "Input sequence: \n",
      "ich habe hunger und koche jetzt\n",
      "\n",
      "Tokenized input data structure: \n",
      "{'input_ids': tensor([[ 277,  865,  315, 7442,  292,  339, 6738, 1333]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "\n",
      "Token IDs and their words: \n",
      "tensor(277) ich\n",
      "tensor(865)  habe\n",
      "tensor(315)  h\n",
      "tensor(7442) unger\n",
      "tensor(292)  und\n",
      "tensor(339)  k\n",
      "tensor(6738) oche\n",
      "tensor(1333)  jetzt\n",
      "\n",
      "All logits for next word: \n",
      "torch.Size([1, 50265])\n",
      "\n",
      "Top 10 predicted token IDs for the next word: \n",
      "[18, 412, 16, 5, 387, 472, 292, 633, 362, 941]\n",
      "\n",
      "Top 10 predicted next words for the sequence: \n",
      "['.', ' nicht', ',', '!', ' für', ' auch', ' und', ' nur', ' zu', ' schon']\n",
      "\n",
      "End demo \n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"\\nBegin next-word prediction using German GPT-2 model\")\n",
    "\n",
    "# Lade den deutschen GPT-2 Tokenizer und das Modell\n",
    "toker = AutoTokenizer.from_pretrained(\"dbmdz/german-gpt2\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"dbmdz/german-gpt2\")\n",
    "\n",
    "# Beispiel-Eingabesequenz auf Deutsch\n",
    "seq = \"ich habe hunger und koche jetzt\"\n",
    "print(\"\\nInput sequence: \")\n",
    "print(seq)\n",
    "\n",
    "# Tokenisiere den Text\n",
    "inpts = toker(seq, return_tensors=\"pt\")\n",
    "print(\"\\nTokenized input data structure: \")\n",
    "print(inpts)\n",
    "\n",
    "# Token-IDs anzeigen\n",
    "inpt_ids = inpts[\"input_ids\"]\n",
    "print(\"\\nToken IDs and their words: \")\n",
    "for id in inpt_ids[0]:\n",
    "    word = toker.decode(id)\n",
    "    print(id, word)\n",
    "\n",
    "# Vorhersage für das nächste Token\n",
    "with torch.no_grad():\n",
    "    logits = model(**inpts).logits[:, -1, :]\n",
    "print(\"\\nAll logits for next word: \")\n",
    "print(logits.shape)\n",
    "\n",
    "# Top-10 wahrscheinlichste Wörter auswählen\n",
    "top_k = 10\n",
    "top_k_probs = torch.topk(logits, top_k)\n",
    "top_k_ids = top_k_probs.indices[0].tolist()\n",
    "\n",
    "print(f\"\\nTop {top_k} predicted token IDs for the next word: \")\n",
    "print(top_k_ids)\n",
    "\n",
    "# Top-10 Wörter dekodieren\n",
    "print(f\"\\nTop {top_k} predicted next words for the sequence: \")\n",
    "top_k_words = [toker.decode(pred_id) for pred_id in top_k_ids]\n",
    "print(top_k_words)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
